#cloud-config
output: { all : '| tee -a /var/log/cloud-init-output.log' }
repo_update: true
repo_upgrade: all
package_upgrade: true
package_reboot_if_required: true

cloud_init_modules:
  - migrator
  - bootcmd
  - write-files
  - growpart
  - resizefs
  - disk_setup
  - mounts
  - set_hostname
  - update_hostname
  - update_etc_hosts
  - ca-certs
  - rsyslog
  - users-groups
  - ssh

# AL2023
packages:
  - bash-completion
  - e2fsprogs
  - ec2-utils
  - git-core
  - vim-enhanced
  - awscli
  - nvme-cli
  - jq
  - htop
  - docker
  - amazon-cloudwatch-agent
  - amazon-ecr-credential-helper

write_files:
  - path: /opt/scripts/ebs-mount.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      device_name=$${device_name:-$1}
      mountpoint=$${mountpoint:-$2}
      service_name=$${service_name:-$3}

      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"
      AVAILABILITY_ZONE="$(ec2-metadata --availability-zone | awk '{print $2}')"
      INSTANCE_ROLE="$(aws ec2 describe-tags --filters Name=resource-id,Values=$(ec2-metadata --instance-id | awk '{print $2}') | jq -r '.Tags[] | select(.Key == "Role") | .Value')"
      echo "AVAILABILITY_ZONE: $AVAILABILITY_ZONE"
      echo "INSTANCE_ROLE: $INSTANCE_ROLE"

      while true; do
          i=1
          while UNATTACHED_VOLUME_ID="$(aws ec2 describe-volumes --filters Name=tag:Service,Values=$service_name Name=tag:AutoAttachGroup,Values=$INSTANCE_ROLE Name=availability-zone,Values=$AVAILABILITY_ZONE | jq -r '.Volumes[] | select(.Attachments | length == 0) | .VolumeId' | shuf -n 1)";
          do
            if [[ -n $UNATTACHED_VOLUME_ID ]] || [[ $i -gt 3 ]]; then break; fi
            echo [$i/3] UNATTACHED_VOLUME_ID is empty, waiting for 2 minutes
            sleep 120 && i=$((i+1))
            continue
          done

          echo "UNATTACHED_VOLUME_ID: $UNATTACHED_VOLUME_ID"

          aws ec2 attach-volume --device "$device_name" --instance-id=$(ec2-metadata --instance-id | awk '{print $2}') --volume-id $UNATTACHED_VOLUME_ID
          if [ "$?" != "0" ]; then
              sleep 10
              continue
          fi

          sleep 30

          ATTACHMENTS_COUNT="$(aws ec2 describe-volumes --filters Name=volume-id,Values=$UNATTACHED_VOLUME_ID | jq -r '.Volumes[0].Attachments | length')"
          if [ "$ATTACHMENTS_COUNT" != "0" ]; then break; fi
      done
      echo 'Waiting for 30 seconds for the disk to become mountable...'
      sleep 30

      mkdir -vp $mountpoint
      export MOUNTED_DEVICE_NAME=$(lsblk -ip --noheadings | awk '{print $1 " " ($7? "MOUNTEDPART" : "") }' | sed ':a;N;$!ba;s/\n`/ /g' | grep -v MOUNTEDPART | grep -v /dev/nvme0n1)
      echo $MOUNTED_DEVICE_NAME
      if mount -o defaults -t ext4 $device_name $mountpoint; then
          echo 'Successfully mounted existing disk'
      else
          echo 'Trying to mount a fresh disk'
          mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard $device_name
          mount -o defaults -t ext4 $device_name $mountpoint && echo 'Successfully mounted a fresh disk'
      fi
      echo "$device_name $mountpoint ext4 defaults,nofail 0 2" | tee -a /etc/fstab
  - path: /root/.docker/config.json
    permissions: '600'
    content: |
      {
        "credsStore": "ecr-login"
      }
  - path: /etc/docker/daemon.json
    permissions: '644'
    content: |
      {
        "features": {
          "containerd-snapshotter": true
        },
        "builder": {
          "gc": {
            "enabled": true,
            "defaultKeepStorage": "${docker_cache_size}GB"
          }
        }
      }
  - path: /opt/scripts/get-docker-extras.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -eu -o pipefail

      # install old docker-compose
      yum install -y libxcrypt-compat
      version=1.29.2
      os=$(uname -s)
      arch=$(uname -m)

      ln -s /usr/share/bash-completion/docker /etc/bash_completion.d/docker

      # install new compose plugin
      version=2.32.0
      plugins_dir=/usr/local/lib/docker/cli-plugins
      mkdir -p $plugins_dir
      curl -SL https://github.com/docker/compose/releases/download/v$version/docker-compose-$${os,,}-$arch -o $plugins_dir/docker-compose
      chmod +x $plugins_dir/docker-compose
  - path: /opt/scripts/mysql-initdb.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # MySQL Database Initialization Script
      # Usage:
      #   /opt/scripts/mysql-initdb.sh /mountpoint           - Normal initialization
      #   /opt/scripts/mysql-initdb.sh --force /mountpoint   - Force reinitialization (removes all data)

      # Parse command line arguments
      FORCE_INIT=false
      while [[ $# -gt 0 ]]; do
        case $1 in
          --force)
            FORCE_INIT=true
            shift
            ;;
          *)
            break
            ;;
        esac
      done

      # Set default paths
      MOUNTPOINT=$${mountpoint:-$1}
      MYSQLDIR=$MOUNTPOINT/mysql
      DATADIR=$MYSQLDIR/data

      ## Create dirs
      if [ ! -d $MYSQLDIR ]; then
        mkdir -p $MYSQLDIR/data
        chown -R mysql:mysql $MYSQLDIR
      fi

      # Get MySQL password from SSM
      SSM_PARAM="${ssm_db_credentials}"
      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"
      CONNECTION_STRING=$(aws ssm get-parameter --name "$SSM_PARAM" --with-decryption --query 'Parameter.Value' --output text)
      DB_PASSWORD=$(echo "$CONNECTION_STRING" | awk -F'[:@]' '{print $3}')
      DB_PORT=$(echo "$CONNECTION_STRING" | awk -F'[:/]' '{print $6}')

      # Check if the password retrieval was successful
      if [ -z "$DB_PASSWORD" ]; then
        echo "Failed to retrieve password from AWS SSM Parameter Store"
        exit 1
      fi

      # Update my.cnf
      CNF=/etc/my.cnf.d/mariadb-server.cnf
      sed -i \
        -e "s#\(datadir=\).*#\1$DATADIR#" \
        -e "/datadir=/i \
      bind-address=0.0.0.0\n\
      port=${db_port}\n\
      ssl-ca=${ssl_certs_dest}/ca.pem\n\
      ssl-cert=${ssl_certs_dest}/server-cert.pem\n\
      ssl-key=${ssl_certs_dest}/server-key.pem" \
      $CNF

      # Setup ssl
      SSL_DIR=${ssl_certs_dest}
      mkdir -p "$SSL_DIR"
      CERTS=( ${ssl_certs} )
      for cert in "$${CERTS[@]}"
      do
        aws ssm get-parameter --name ${ssl_certs_ssm_prefix}/$cert --with-decryption --query 'Parameter.Value' --output text > $SSL_DIR/$cert
      done

      chown -R mysql:mysql $SSL_DIR
      chmod 600 $SSL_DIR/*

      # Create log directory
      mkdir -p /var/log/mysql
      chown -R mysql:mysql /var/log/mysql

      # Reload systemd
      systemctl daemon-reload

      # Start and enable MySQL
      systemctl enable --now mariadb

      # Force reinitialization if --force flag is specified
      if [ "$FORCE_INIT" = true ]; then
        echo "Force initialization requested. Removing existing data..."
        systemctl stop mariadb
        rm -rf "$DATADIR"/*
        rm -f "$DATADIR/.mysql_initialized"
        systemctl start mariadb
      fi

      # Set root password and create user/database
      if [ ! -f "$DATADIR/.mysql_initialized" ]; then
        # Secure the MySQL installation
        mysql --user=root <<EOF
      -- Set root password
      ALTER USER 'root'@'localhost' IDENTIFIED BY '$DB_PASSWORD';
      -- Create application database
      CREATE DATABASE IF NOT EXISTS ${db_name};
      -- Create application user and grant privileges
      CREATE USER IF NOT EXISTS '${db_username}'@'%' IDENTIFIED BY '$DB_PASSWORD';
      GRANT ALL ON ${db_name}.* TO '${db_username}'@'%';
      -- Allow remote connections
      CREATE USER IF NOT EXISTS '${db_username}'@'localhost' IDENTIFIED BY '$DB_PASSWORD';
      GRANT ALL ON ${db_name}.* TO '${db_username}'@'localhost';
      -- Remove anonymous users
      DELETE FROM mysql.user WHERE User='';
      -- Disallow root login from remote hosts
      DELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1');
      -- Drop the test database if it exists
      DROP DATABASE IF EXISTS test;
      -- Remove privileges on test databases
      DELETE FROM mysql.db WHERE Db='test' OR Db='test\\_%';
      -- Apply changes
      FLUSH PRIVILEGES;
      EOF
        # Mark as initialized
        touch $DATADIR/.mysql_initialized
        echo "MySQL has been initialized with application user and database."
      fi

  - path: /opt/scripts/mysql.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # Get MySQL password from SSM
      SSM_PARAM="${ssm_db_credentials}"
      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"
      CONNECTION_STRING="$${1:-$(aws ssm get-parameter --name $SSM_PARAM --with-decryption --query 'Parameter.Value' --output text)}"

      # Remove the mysql:// prefix
      CONNECTION_STRING="$${CONNECTION_STRING#mysql://}"

      # Extract user
      user="$${CONNECTION_STRING%%:*}"
      rest="$${CONNECTION_STRING#*:}"

      # Extract password
      pass="$${rest%%@*}"
      rest="$${rest#*@}"

      # Extract host
      host="$${rest%%:*}"
      rest="$${rest#*:}"

      # Extract port
      port="$${rest%%/*}"

      # Extract database name
      dbname="$${rest#*/}"

      # Connect to MySQL using extracted parameters
      mariadb -h "$host" -P "$port" -u "$user" -p"$pass" "$dbname"
  - path: /opt/scripts/mysql-backup.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # MySQL Backup and Restore Script
      # Usage:
      #   /opt/scripts/mysql-backup.sh --create                                    - Create backup with current date
      #   /opt/scripts/mysql-backup.sh --create /path/to/file/custom.sql.gz        - Create backup to custom S3 path
      #   /opt/scripts/mysql-backup.sh --list                                      - List available backups
      #   /opt/scripts/mysql-backup.sh --restore                                   - Restore latest backup
      #   /opt/scripts/mysql-backup.sh --restore 2023-12-25                        - Restore backup from specific date
      #   /opt/scripts/mysql-backup.sh --restore /path/to/file/custom.sql.gz       - Restore specific backup file

      # Configuration
      SERVICE="${service_name}"
      DB_USER="${db_backup_user}"
      SSM_PARAM="${ssm_db_credentials}"
      S3_BUCKET="${db_backup_bucket}"
      BACKUP_DIR="${db_backup_dir}"
      DATA_MOUNTPOINT="${data_mountpoint}"

      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"

      # Parse command line arguments
      if [[ $# -eq 0 ]]; then
        # No arguments - default to create
        COMMAND="--create"
        ARGUMENT=""
      else
        # Parse arguments
        case "$1" in
          --create)
            COMMAND="--create"
            ARGUMENT="$${2:-}"
            ;;
          --list)
            COMMAND="--list"
            ARGUMENT=""
            ;;
          --restore)
            COMMAND="--restore"
            ARGUMENT="$${2:-}"
            ;;
          *)
            echo "Error: Unknown command $1"
            echo "Use --create [path], --list, or --restore [YYYY-MM-DD|path]"
            exit 1
            ;;
        esac
      fi

      # Common function to get database credentials
      get_db_credentials() {
        echo "Fetching connection string from AWS SSM Parameter Store..."
        CONNECTION_STRING=$(aws ssm get-parameter --name "$SSM_PARAM" --with-decryption --query 'Parameter.Value' --output text)
        PASSWORD=$(echo "$CONNECTION_STRING" | awk -F'[:@]' '{print $3}')

        if [ -z "$PASSWORD" ]; then
          echo "Error: failed to retrieve password from SSM Parameter Store."
          exit 1
        fi
      }

      # Function to create backup
      create_backup() {
        local custom_path="$1"
        get_db_credentials

        mkdir -p "$BACKUP_DIR"

        if [[ -n "$custom_path" ]]; then
          # Custom S3 path provided
          S3_PATH="s3://$S3_BUCKET/$custom_path"
          BACKUP_FILE=$(basename "$custom_path")
        else
          # Default path with current date
          TIMESTAMP=$(date +'%Y/%m/%d')
          BACKUP_FILE="mysql_all_databases_$(date +%Y%m%d_%H%M%S).sql.gz"
          S3_PATH="s3://$S3_BUCKET/$SERVICE/$TIMESTAMP/$BACKUP_FILE"
        fi

        echo "Creating backup of all MySQL databases..."
        mysqldump --user=root --password="$PASSWORD" --all-databases --single-transaction --quick --lock-tables=false | gzip > "$BACKUP_DIR/$BACKUP_FILE"

        if [ $? -ne 0 ]; then
          echo "Error while creating backup."
          exit 1
        fi

        echo "Uploading backup to $S3_PATH..."
        aws s3 cp "$BACKUP_DIR/$BACKUP_FILE" "$S3_PATH"

        if [ $? -eq 0 ]; then
          echo "Backup successfully uploaded to $S3_PATH"
        else
          echo "Error uploading backup to S3"
          exit 1
        fi

        rm -f "$BACKUP_DIR/$BACKUP_FILE"
        echo "Backup process completed."
      }

      # Function to list backups
      list_backups() {
        echo "Available MySQL backups in s3://$S3_BUCKET/$SERVICE/"
        echo "Date       Size     Time"
        echo "---------- -------- --------"
        aws s3 ls "s3://$S3_BUCKET/$SERVICE/" --recursive | grep "\.sql\.gz$" | \
        awk '{
          # Extract date from path like SERVICE/2023/12/25/file.sql.gz
          split($4, path_parts, "/")
          if (length(path_parts) >= 4) {
            iso_date = path_parts[2] "-" path_parts[3] "-" path_parts[4]
            dir_key = iso_date
            filename = path_parts[length(path_parts)]

            # Accumulate total size per directory
            dir_sizes[dir_key] += $3

            # Extract time from first file in directory (latest time wins)
            if (match(filename, /_([0-9]{8})_([0-9]{6})\.sql\.gz$/, time_parts)) {
              time_str = substr(time_parts[2], 1, 2) ":" substr(time_parts[2], 3, 2) ":" substr(time_parts[2], 5, 2)
              dir_times[dir_key] = time_str
            }
          }
        }
        END {
          # Print accumulated results
          for (dir in dir_sizes) {
            size = dir_sizes[dir]
            time_str = dir_times[dir] ? dir_times[dir] : "unknown"

            # Format size with units
            if (size >= 1073741824) {
              size_fmt = sprintf("%.1fGB", size/1073741824)
            } else if (size >= 1048576) {
              size_fmt = sprintf("%.1fMB", size/1048576)
            } else if (size >= 1024) {
              size_fmt = sprintf("%.1fKB", size/1024)
            } else {
              size_fmt = sprintf("%dB", size)
            }

            printf "%-10s %-8s %s\n", dir, size_fmt, time_str
          }
        }' | sort
      }

      # Function to restore backup
      restore_backup() {
        local restore_path="$1"
        get_db_credentials

        mkdir -p "$BACKUP_DIR"

        if [[ -n "$restore_path" ]]; then
          if [[ "$restore_path" =~ ^[0-9]{4}-[0-9]{2}-[0-9]{2}$ ]]; then
            # Date format YYYY-MM-DD
            formatted_date=$(echo "$restore_path" | awk -F'-' '{printf "%s/%02d/%02d", $1, $2, $3}')
            echo "Looking for backups from date: $formatted_date"
            echo "Searching in: s3://$S3_BUCKET/$SERVICE/$formatted_date/"

            # List what's available for debugging
            echo "Available files in this date directory:"
            aws s3 ls "s3://$S3_BUCKET/$SERVICE/$formatted_date/" || echo "  (directory not found or empty)"

            # Find the latest backup from that date
            LATEST_BACKUP=$(aws s3 ls "s3://$S3_BUCKET/$SERVICE/$formatted_date/" | grep "\.sql\.gz$" | sort | tail -n 1 | awk '{print $4}')

            if [[ -z "$LATEST_BACKUP" ]]; then
              echo ""
              echo "No backup found for date $restore_path"
              echo ""
              echo "Available backup dates:"
              /opt/scripts/mysql-backup.sh --list
              exit 1
            fi

            S3_PATH="s3://$S3_BUCKET/$SERVICE/$formatted_date/$LATEST_BACKUP"
            LOCAL_FILE="$BACKUP_DIR/$LATEST_BACKUP"
          else
            # Custom S3 path
            S3_PATH="s3://$S3_BUCKET/$restore_path"
            LOCAL_FILE="$BACKUP_DIR/$(basename "$restore_path")"
          fi
        else
          # Find the latest backup
          echo "Finding latest backup..."
          LATEST_BACKUP=$(aws s3 ls "s3://$S3_BUCKET/$SERVICE/" --recursive | grep "\.sql\.gz$" | sort | tail -n 1 | awk '{print $4}')

          if [[ -z "$LATEST_BACKUP" ]]; then
            echo "No backups found"
            exit 1
          fi

          S3_PATH="s3://$S3_BUCKET/$LATEST_BACKUP"
          LOCAL_FILE="$BACKUP_DIR/$(basename "$LATEST_BACKUP")"
        fi

        echo "Downloading backup from $S3_PATH..."
        aws s3 cp "$S3_PATH" "$LOCAL_FILE"

        if [ $? -ne 0 ]; then
          echo "Error downloading backup from S3"
          exit 1
        fi

        echo "Force reinitializing MySQL database..."
        /opt/scripts/mysql-initdb.sh --force "$DATA_MOUNTPOINT"

        echo "Restoring backup..."
        zcat "$LOCAL_FILE" | mysql --user=root --password="$PASSWORD"

        if [ $? -ne 0 ]; then
          echo "Error restoring backup"
          exit 1
        fi

        rm -f "$LOCAL_FILE"
        echo "Restore completed successfully."
      }

      # Execute command
      case "$COMMAND" in
        --create)
          create_backup "$ARGUMENT"
          ;;
        --list)
          list_backups
          ;;
        --restore)
          restore_backup "$ARGUMENT"
          ;;
        *)
          echo "Error: Unknown command $COMMAND"
          exit 1
          ;;
      esac

  - path: /etc/systemd/system/mysql-backup.service
    permissions: '0644'
    content: |
      [Unit]
      Description=MySQL Database Backup

      [Service]
      Type=oneshot
      ExecStart=/opt/scripts/mysql-backup.sh

  - path: /etc/systemd/system/mysql-backup.timer
    permissions: '0644'
    content: |
      [Unit]
      Description=MySQL Backup Timer (8:00 - 10:00 AM Window)

      [Timer]
      OnCalendar=*-*-* 08:00:00
      RandomizedDelaySec=2h
      Persistent=true

      [Install]
      WantedBy=timers.target

  - path: /opt/scripts/deploy-app-in-docker.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      WORKDIR=$${WORKDIR:-${data_mountpoint}/code}
      GIT_REPOSITORY=$${GIT_REPOSITORY:-${git_repository}}
      COMPOSE_OVERRIDES=$${COMPOSE_OVERRIDES:-${compose_overrides}}

      GITHUB_REF_NAME="$${BRANCH:-$${GITHUB_REF_NAME:-main}}"
      GITHUB_SHA="$${COMMIT:-$${GITHUB_SHA:-}}"
      GITHUB_WORKFLOW=$${GITHUB_WORKFLOW:-manual}
      GITHUB_RUN_NUMBER=$${GITHUB_RUN_NUMBER:-$(date +%F_%H-%M-%S)}

      # enable build cache
      ENABLE_CACHE=${ENABLE_CACHE:-true}
      CACHE_DIR=${data_mountpoint}/docker-cache

      # configure cloudwatch function
      ENABLE_CLOUDWATCH=$${ENABLE_CLOUDWATCH:-true}
      CW_STREAM_NAME=$GITHUB_WORKFLOW/$GITHUB_REF_NAME/$GITHUB_RUN_NUMBER
      CW_STREAM_NAME_SANITIZED=$(sed -E 's/[^a-zA-Z0-9_-]/_/g' <<<"$CW_STREAM_NAME")
      CW_CFG_PATH=${data_mountpoint}/cloudwatch/cloudwatch.json
      CW_LOG_PATH=${data_mountpoint}/logs/$CW_STREAM_NAME_SANITIZED.log
      CW_AGENT_BIN=/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl
      CW_CFG_TEMPLATE=$(/opt/scripts/get-cloudwatch-config.sh)

      setup_ssh_config() {
        local repo_deploy_key=$(aws ssm get-parameter --name "${ssm_repo_deploy_key}" --with-decryption --query 'Parameter.Value' --output text)
        mkdir -p ~/.ssh
        chmod 700 ~/.ssh
        echo -e "$repo_deploy_key" > ~/.ssh/id_rsa
        chmod 600 ~/.ssh/id_rsa
        ssh-keyscan -t rsa $(echo "$GIT_REPOSITORY" | grep -oP '(?<=@)[^:]+') > ~/.ssh/known_hosts
      }

      sync_sources() {
        # clone repo if WORKDIR is not a valid git repo
        git -C $WORKDIR rev-parse 2>/dev/null || git clone $GIT_REPOSITORY $WORKDIR

        cd $WORKDIR

        # Fetch all remote refs and clean up deleted branches
        git fetch --all --prune

        # Checkout remote branch or tag
        if git show-ref --verify "refs/remotes/origin/$GITHUB_REF_NAME"; then
          git checkout -B "$GITHUB_REF_NAME" "origin/$GITHUB_REF_NAME"
        elif git show-ref --tags | grep -q "refs/tags/$GITHUB_REF_NAME"; then
          git checkout "tags/$GITHUB_REF_NAME"
        fi

        # Delete all local branches except the current one
        local branches_count=$(git branch | wc -l)
        if [[ $branches_count -gt 1 ]]; then
          git branch | grep -v '^\*' | xargs -r git branch -D
        fi

        # Reset working directory to clean state
        git reset --hard
        git clean -xfd

        # Reset to specific commit if provided
        if [[ -n $GITHUB_SHA ]]; then
          git reset --hard "$GITHUB_SHA"
        fi
      }

      configure_host() {
        [[ -s .deploy/ec2-configure-host.sh ]] && .deploy/ec2-configure-host.sh
      }

      prepare_docker_aws() {
        ECR_REGISTRY=$(aws ecr get-authorization-token --query authorizationData[0].proxyEndpoint --output text | cut -d/ -f3)

        if [[ -f /root/.docker/config.json ]] && ! grep -wq 'ecr-login' /root/.docker/config.json; then
          aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_REGISTRY
        fi
      }

      prepare_docker_overrides() {
        [[ -z "$${COMPOSE_OVERRIDES:-}" ]] && { echo "COMPOSE_OVERRIDES not set, skipping"; return 0; }

        for file in $${COMPOSE_OVERRIDES[@]}; do
          [[ ! -s "$file" ]] && { echo "File $file not found, skipping"; return 0; }
        done

        docker run --rm -v "$PWD":/workdir mikefarah/yq ea '. as $item ireduce ({}; . * $item)' \
          $${COMPOSE_OVERRIDES[@]} > docker-compose.override.yml
      }

      setup_env_file() {
        [[ -s .env ]] && mv .env .env.bak

        ENV_FILES="${ssm_env_files}"
        for env_file in $ENV_FILES; do
          aws ssm get-parameter --name "$env_file" --with-decryption --query 'Parameter.Value' --output text >> .env
        done
      }

      run_docker() {
        docker compose pull --ignore-pull-failures -q
        docker compose build -q
        docker compose up -d
      }

      setup_cloudwatch() {
        [[ $ENABLE_CLOUDWATCH == true ]] || return 0

        if [[ ! -f "$CW_AGENT_BIN" ]]; then
          echo "[WARN] CloudWatch agent binary not found at $CW_AGENT_BIN, skipping setup"
          return 0
        fi

        mkdir -p $(dirname $CW_CFG_PATH $CW_LOG_PATH | xargs)
        jq \
          --arg log_stream_name "$CW_STREAM_NAME" \
          '.logs.logs_collected.files.collect_list[0] += {
            log_stream_name: $log_stream_name
          }' <<<"$CW_CFG_TEMPLATE" > $CW_CFG_PATH

        # delete old log files
        local log_dir=$(dirname $CW_LOG_PATH)
        [[ -d $log_dir ]] && rm -f $log_dir/*

        # Redirect stdout/stderr to log file
        exec > >( tee -a "$CW_LOG_PATH" )
        exec 2>&1
        $CW_AGENT_BIN \
          -a fetch-config \
          -m ec2 \
          -c file:"$CW_CFG_PATH" \
          -s
      }

      setup_cloudwatch
      setup_ssh_config
      sync_sources
      configure_host
      prepare_docker_aws
      prepare_docker_overrides
      setup_env_file
      run_docker
  - path: /opt/scripts/get-cloudwatch-config.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail
      SSM_PARAM=${ssm_cloudwatch_config}
      aws ssm get-parameter \
        --name "$SSM_PARAM" \
        --with-decryption \
        --query 'Parameter.Value' \
        --output text
runcmd:
  # install aws ssm agent
  - yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm
  - |
    jq --arg CommandWorkersLimit 1 '
      .Mds += { CommandWorkersLimit: ($CommandWorkersLimit | tonumber) } |
      .Agent.SelfUpdate = true
    ' /etc/amazon/ssm/amazon-ssm-agent.json.template > /etc/amazon/ssm/amazon-ssm-agent.json
  - systemctl enable --now --no-block amazon-ssm-agent
  # dist-upgrade
  - dnf upgrade -y --refresh --releasever=latest
  # mounting ebs disks
  - /opt/scripts/ebs-mount.sh ${data_device_name} ${data_mountpoint} ${data_service_name}
  - /opt/scripts/ebs-mount.sh ${backup_device_name} ${backup_mountpoint} ${backup_service_name}
  # download docker-compose
  - systemctl enable --now --no-block --force docker
  - /opt/scripts/get-docker-extras.sh
  - systemctl disable --now --no-block --force docker
  # initialize mysql server (separated from "packages:" block)
  - yum install -y mariadb105-server
  - /opt/scripts/mysql-initdb.sh ${data_mountpoint}
  - systemctl enable --now --no-block --force mariadb
  # enable mysql backup
  - systemctl daemon-reload
  - systemctl enable --now --no-block mysql-backup.timer
  # deploy application
  # disabled to speed up instance start
  #- /opt/scripts/deploy-app-in-docker.sh
