#cloud-config
output: { all : '| tee -a /var/log/cloud-init-output.log' }
repo_update: true
repo_upgrade: all
package_upgrade: true
package_reboot_if_required: true

cloud_init_modules:
  - migrator
  - bootcmd
  - write-files
  - growpart
  - resizefs
  - disk_setup
  - mounts
  - set_hostname
  - update_hostname
  - update_etc_hosts
  - ca-certs
  - rsyslog
  - users-groups
  - ssh

# AL2023
packages:
  - bash-completion
  - e2fsprogs
  - ec2-utils
  - git-core
  - vim-enhanced
  - awscli
  - nvme-cli
  - jq
  - htop
  - docker
  - cronie
  - pv

write_files:
  - path: /opt/scripts/ebs-mount.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      device_name=$${device_name:-$1}
      mountpoint=$${mountpoint:-$2}
      service_name=$${service_name:-$3}

      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"
      AVAILABILITY_ZONE="$(ec2-metadata --availability-zone | awk '{print $2}')"
      INSTANCE_ROLE="$(aws ec2 describe-tags --filters Name=resource-id,Values=$(ec2-metadata --instance-id | awk '{print $2}') | jq -r '.Tags[] | select(.Key == "Role") | .Value')"
      echo "AVAILABILITY_ZONE: $AVAILABILITY_ZONE"
      echo "INSTANCE_ROLE: $INSTANCE_ROLE"

      while true; do
          i=1
          while UNATTACHED_VOLUME_ID="$(aws ec2 describe-volumes --filters Name=tag:Service,Values=$service_name Name=tag:AutoAttachGroup,Values=$INSTANCE_ROLE Name=availability-zone,Values=$AVAILABILITY_ZONE | jq -r '.Volumes[] | select(.Attachments | length == 0) | .VolumeId' | shuf -n 1)";
          do
            if [[ -n $UNATTACHED_VOLUME_ID ]] || [[ $i -gt 3 ]]; then break; fi
            echo [$i/3] UNATTACHED_VOLUME_ID is empty, waiting for 2 minutes
            sleep 120 && i=$((i+1))
            continue
          done

          echo "UNATTACHED_VOLUME_ID: $UNATTACHED_VOLUME_ID"

          aws ec2 attach-volume --device "$device_name" --instance-id=$(ec2-metadata --instance-id | awk '{print $2}') --volume-id $UNATTACHED_VOLUME_ID
          if [ "$?" != "0" ]; then
              sleep 10
              continue
          fi

          sleep 30

          ATTACHMENTS_COUNT="$(aws ec2 describe-volumes --filters Name=volume-id,Values=$UNATTACHED_VOLUME_ID | jq -r '.Volumes[0].Attachments | length')"
          if [ "$ATTACHMENTS_COUNT" != "0" ]; then break; fi
      done
      echo 'Waiting for 30 seconds for the disk to become mountable...'
      sleep 30

      mkdir -vp $mountpoint
      export MOUNTED_DEVICE_NAME=$(lsblk -ip --noheadings | awk '{print $1 " " ($7? "MOUNTEDPART" : "") }' | sed ':a;N;$!ba;s/\n`/ /g' | grep -v MOUNTEDPART | grep -v /dev/nvme0n1)
      echo $MOUNTED_DEVICE_NAME
      if mount -o defaults -t ext4 $device_name $mountpoint; then
          echo 'Successfully mounted existing disk'
      else
          echo 'Trying to mount a fresh disk'
          mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard $device_name
          mount -o defaults -t ext4 $device_name $mountpoint && echo 'Successfully mounted a fresh disk'
      fi
      echo "$device_name $mountpoint ext4 defaults,nofail 0 2" | tee -a /etc/fstab
  - path: /opt/scripts/get-docker-extras.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -eu -o pipefail

      # install old docker-compose
      yum install -y libxcrypt-compat
      version=1.29.2
      os=$(uname -s)
      arch=$(uname -m)

      ln -s /usr/share/bash-completion/docker /etc/bash_completion.d/docker

      # install new compose plugin
      version=2.32.0
      plugins_dir=/usr/local/lib/docker/cli-plugins
      mkdir -p $plugins_dir
      curl -SL https://github.com/docker/compose/releases/download/v$version/docker-compose-$${os,,}-$arch -o $plugins_dir/docker-compose
      chmod +x $plugins_dir/docker-compose

  - path: /opt/scripts/postgresql-initdb.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # PostgreSQL Database Initialization Script
      # Usage:
      #   /opt/scripts/postgresql-initdb.sh /mountpoint           - Normal initialization
      #   /opt/scripts/postgresql-initdb.sh --force /mountpoint   - Force reinitialization (removes all data)

      # Parse command line arguments
      FORCE_INIT=false
      while [[ $# -gt 0 ]]; do
        case $1 in
          --force)
            FORCE_INIT=true
            shift
            ;;
          *)
            break
            ;;
        esac
      done

      # Set default paths
      MOUNTPOINT=$${mountpoint:-$1}
      PGDIR=$MOUNTPOINT/pgsql
      DATADIR=$PGDIR/data

      # Create dirs
      if [ ! -d $PGDIR ]; then
        mkdir -p $PGDIR
        chown "postgres:" $PGDIR
      fi

      # Create systemd unit file
      if [ ! -s /etc/systemd/system/postgresql@${stage}.service.d/30-postgresql-setup.conf ]; then
        postgresql-new-systemd-unit --unit=postgresql@${stage} --datadir=$DATADIR
      fi

      # Force reinitialization if --force flag is specified
      if [ "$FORCE_INIT" = true ] && [ -d $DATADIR ]; then
        echo "Force initialization requested. Removing existing data directory..."
        systemctl stop postgresql@${stage}
        rm -rf "$DATADIR"
      fi

      if [ -d $DATADIR ]; then
        echo "Datadir '$DATADIR' already exists, exiting..."
        exit 0
      fi

      SSM_PARAM="${ssm_db_credentials}"
      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"

      # Fetch the connection string from AWS SSM Parameter Store
      CONNECTION_STRING=$(aws ssm get-parameter --name "$SSM_PARAM" --with-decryption --query 'Parameter.Value' --output text)

      # Extract password and port from the connection string
      PASSWORD=$(echo "$CONNECTION_STRING" | awk -F'[:@]' '{print $3}')
      PORT=$(echo "$CONNECTION_STRING" | awk -F'[:/]' '{print $6}')

      # Check if the password retrieval was successful
      if [ -z "$PASSWORD" ]; then
        echo "Failed to retrieve password from AWS Secrets Manager"
        exit 1
      fi

      # Create a temporary file for the password
      PWFILE=$(mktemp)
      echo "$PASSWORD" > "$PWFILE"
      # Set secure permissions on the file
      chmod 600 "$PWFILE"
      chown "postgres:" "$PWFILE"

      # Initialize PostgreSQL with the password file
      export PGSETUP_INITDB_OPTIONS="--pgdata=$DATADIR --pwfile=$PWFILE"
      postgresql-setup --unit postgresql@${stage} --port $PORT --initdb

      # Set the server configuration
      sed -i "s/^#\(listen_addresses = \)\('localhost'\)/\1'*'/" $DATADIR/postgresql.conf

      # Setup ssl certificates
      SSL_DIR=${ssl_certs_dest}
      mkdir -p "$SSL_DIR"
      CERTS=( ${ssl_certs} )
      for cert in "$${CERTS[@]}"
      do
        aws ssm get-parameter --name ${ssl_certs_ssm_prefix}/$cert --with-decryption --query 'Parameter.Value' --output text > $SSL_DIR/$cert
      done

      chown -R postgres:postgres $SSL_DIR
      chmod 600 $SSL_DIR/*

      # SSL configuration
      echo "ssl = on" >> $DATADIR/postgresql.conf
      echo "ssl_ca_file = '${ssl_certs_dest}/ca.pem'" >> $DATADIR/postgresql.conf
      echo "ssl_cert_file = '${ssl_certs_dest}/server-cert.pem'" >> $DATADIR/postgresql.conf
      echo "ssl_key_file = '${ssl_certs_dest}/server-key.pem'" >> $DATADIR/postgresql.conf

      # Allow SSL-only network authorization for user postgres
      echo "hostssl all             postgres        ${db_allowed_network}           scram-sha-256" >> $DATADIR/pg_hba.conf
      echo "hostssl ${db_name}             ${db_username}        ${db_allowed_network}           scram-sha-256" >> $DATADIR/pg_hba.conf
      echo "hostssl ${db_name}             ${db_username}        172.16.0.0/12           scram-sha-256" >> $DATADIR/pg_hba.conf

      # start postgresql server
      systemctl enable --now --no-block --force postgresql@${stage}

      # Create the application database and set permissions
      echo "CREATE DATABASE ${db_name};" | su - postgres -c "psql"
      echo "CREATE USER ${db_username} WITH PASSWORD '$(cat $PWFILE)';" |  su - postgres -c "psql"
      echo "GRANT ALL PRIVILEGES ON DATABASE ${db_name} TO ${db_username};" |  su - postgres -c "psql"
      echo "ALTER DATABASE ${db_name} OWNER TO ${db_username};" | su - postgres -c "psql"
      echo "ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO ${db_username};" | su - postgres -c "psql ${db_name}"

      # Clean up the password file
      rm -f "$PWFILE"

      echo "PostgreSQL initialization completed."

  - path: /opt/scripts/postgresql-backup.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # PostgreSQL Backup and Restore Script
      # Usage:
      #   /opt/scripts/postgresql-backup.sh --create                                    - Create backup with current date
      #   /opt/scripts/postgresql-backup.sh --create /path/to/dir/                      - Create backup to custom S3 directory
      #   /opt/scripts/postgresql-backup.sh --list                                      - List available backups
      #   /opt/scripts/postgresql-backup.sh --restore                                   - Restore latest backup
      #   /opt/scripts/postgresql-backup.sh --restore 2023-12-25                        - Restore backup from specific date
      #   /opt/scripts/postgresql-backup.sh --restore /path/to/dir/                     - Restore specific backup directory

      # Configuration
      SERVICE="${service_name}"
      DB_USER="${db_backup_user}"
      SSM_PARAM="${ssm_db_credentials}"
      S3_BUCKET="${db_backup_bucket}"
      BACKUP_DIR="${db_backup_dir}"
      DATA_MOUNTPOINT="${data_mountpoint}"

      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"

      # Parse command line arguments
      if [[ $# -eq 0 ]]; then
        # No arguments - default to create
        COMMAND="--create"
        ARGUMENT=""
      else
        # Parse arguments
        case "$1" in
          --create)
            COMMAND="--create"
            ARGUMENT="$${2:-}"
            ;;
          --list)
            COMMAND="--list"
            ARGUMENT=""
            ;;
          --restore)
            COMMAND="--restore"
            ARGUMENT="$${2:-}"
            ;;
          *)
            echo "Error: Unknown command $1"
            echo "Use --create [path], --list, or --restore [YYYY-MM-DD|path]"
            exit 1
            ;;
        esac
      fi

      # Common function to get database credentials and setup
      get_db_credentials() {
        echo "Fetching connection string from AWS SSM Parameter Store..."
        CONNECTION_STRING=$(aws ssm get-parameter --name "$SSM_PARAM" --with-decryption --query 'Parameter.Value' --output text)
        PASSWORD=$(echo "$CONNECTION_STRING" | awk -F'[:@]' '{print $3}')

        if [ -z "$PASSWORD" ]; then
          echo "Error: failed to retrieve password from SSM Parameter Store."
          exit 1
        fi

        # Get our primary IP address
        IP=$(ip route get 1.1.1.1 | grep -oP '(?<=src )\S+')

        # Create a temporary file for storing the password
        PGPASSFILE=$(mktemp)
        echo "$IP:*:*:$DB_USER:$PASSWORD" > "$PGPASSFILE"
        chmod 600 "$PGPASSFILE"
        export PGPASSFILE
      }

      # Function to create backup
      create_backup() {
        local custom_path="$1"
        get_db_credentials

        mkdir -p "$BACKUP_DIR"

        if [[ -n "$custom_path" ]]; then
          # Custom S3 path provided (should be directory)
          S3_BASE_PATH="s3://$S3_BUCKET/$custom_path"
          TIMESTAMP_DIR=$(date +%Y%m%d_%H%M%S)
        else
          # Default path with current date
          TIMESTAMP=$(date +'%Y/%m/%d')
          TIMESTAMP_DIR=$(date +%Y%m%d_%H%M%S)
          S3_BASE_PATH="s3://$S3_BUCKET/$SERVICE/$TIMESTAMP"
        fi

        # Backup global roles and privileges
        echo "Creating a backup of global roles and privileges..."
        GLOBAL_BACKUP_FILE="globals_roles_privileges_backup.sql.gz"
        pg_dumpall -h $IP -U $DB_USER --globals-only | gzip > "$BACKUP_DIR/$GLOBAL_BACKUP_FILE"

        if [ $? -ne 0 ]; then
          echo "Error while creating backup of global roles and privileges."
          exit 1
        fi

        # Upload global backup to S3
        GLOBAL_S3_PATH="$S3_BASE_PATH/$GLOBAL_BACKUP_FILE"
        echo "Uploading global backup to $GLOBAL_S3_PATH..."
        aws s3 cp "$BACKUP_DIR/$GLOBAL_BACKUP_FILE" "$GLOBAL_S3_PATH"

        # Fetch the list of all databases (excluding postgres database)
        DB_LIST=$(psql -h $IP -U $DB_USER -d postgres -t -c "SELECT datname FROM pg_database WHERE datistemplate = false AND datname != 'postgres';" | tr -d ' ')

        # Backup each database
        for DB_NAME in $DB_LIST; do
          echo "Creating a backup for database '$DB_NAME'..."
          BACKUP_FILE="$${DB_NAME}.dump"
          pg_dump -h $IP -U $DB_USER -F c --create -d $DB_NAME > "$BACKUP_DIR/$BACKUP_FILE"

          if [ $? -ne 0 ]; then
            echo "Error while creating backup for database '$DB_NAME'."
            exit 1
          fi

          # Upload database backup to S3
          S3_PATH="$S3_BASE_PATH/$BACKUP_FILE"
          echo "Uploading backup of database '$DB_NAME' to $S3_PATH..."
          aws s3 cp "$BACKUP_DIR/$BACKUP_FILE" "$S3_PATH"

          if [ $? -eq 0 ]; then
            echo "Backup of database '$DB_NAME' successfully uploaded to $S3_PATH"
          else
            echo "Error uploading backup of database '$DB_NAME' to S3"
            exit 1
          fi
        done

        # Clean up temporary backup files and password file
        rm -f "$BACKUP_DIR"/*.dump
        rm -f "$BACKUP_DIR"/*.gz
        rm -f "$PGPASSFILE"
        echo "Backup process completed."
      }

      # Function to list backups
      list_backups() {
        echo "Available PostgreSQL backup directories in s3://$S3_BUCKET/$SERVICE/"
        echo "Date       Size     Backup Set"
        echo "---------- -------- ----------"
        aws s3 ls "s3://$S3_BUCKET/$SERVICE/" --recursive | grep "\.dump$" | \
        awk '{
          # Extract date from path like SERVICE/2023/12/25/file.dump
          split($4, path_parts, "/")
          if (length(path_parts) >= 4) {
            iso_date = path_parts[2] "-" path_parts[3] "-" path_parts[4]
            dir_key = iso_date

            # Accumulate total size per directory
            dir_sizes[dir_key] += $3
          }
        }
        END {
          # Print accumulated results
          for (dir in dir_sizes) {
            size = dir_sizes[dir]

            # Format size with units
            if (size >= 1073741824) {
              size_fmt = sprintf("%.1fGB", size/1073741824)
            } else if (size >= 1048576) {
              size_fmt = sprintf("%.1fMB", size/1048576)
            } else if (size >= 1024) {
              size_fmt = sprintf("%.1fKB", size/1024)
            } else {
              size_fmt = sprintf("%dB", size)
            }

            printf "%-10s %-8s %s\n", dir, size_fmt, dir
          }
        }' | sort
      }

      # Function to restore backup
      restore_backup() {
        local restore_path="$1"
        get_db_credentials

        mkdir -p "$BACKUP_DIR"

        if [[ -n "$restore_path" ]]; then
          if [[ "$restore_path" =~ ^[0-9]{4}-[0-9]{2}-[0-9]{2}$ ]]; then
            # Date format YYYY-MM-DD
            formatted_date=$(echo "$restore_path" | awk -F'-' '{printf "%s/%02d/%02d", $1, $2, $3}')
            echo "Looking for backups from date: $formatted_date"
            echo "Searching in: s3://$S3_BUCKET/$SERVICE/$formatted_date/"

            # List what's available for debugging
            echo "Available files in this date directory:"
            aws s3 ls "s3://$S3_BUCKET/$SERVICE/$formatted_date/" || echo "  (directory not found or empty)"

            # Check if backup files exist in this date directory
            BACKUP_EXISTS=$(aws s3 ls "s3://$S3_BUCKET/$SERVICE/$formatted_date/" | grep "\.dump$")

            if [[ -z "$BACKUP_EXISTS" ]]; then
              echo "No backup found for date $restore_path"
              echo "Available backup dates:"
              /opt/scripts/postgresql-backup.sh --list
              exit 1
            fi

            echo "Found backup for date $restore_path"
            S3_BASE_PATH="s3://$S3_BUCKET/$SERVICE/$formatted_date"
          else
            # Custom S3 path
            S3_BASE_PATH="s3://$S3_BUCKET/$restore_path"
          fi
        else
          # Find the latest backup
          echo "Finding latest backup..."
          LATEST_BACKUP_PATH=$(aws s3 ls "s3://$S3_BUCKET/$SERVICE/" --recursive | grep "\.dump$" | sort | tail -n 1 | awk '{print $4}')

          if [[ -z "$LATEST_BACKUP_PATH" ]]; then
            echo "No backups found"
            exit 1
          fi

          # Extract directory path from file path
          S3_BASE_PATH="s3://$S3_BUCKET/$(dirname "$LATEST_BACKUP_PATH")"
        fi

        echo "Downloading backups from $S3_BASE_PATH..."

        # Download all backup files from the directory
        aws s3 sync "$S3_BASE_PATH/" "$BACKUP_DIR/" --exclude "*" --include "*.dump" --include "*.sql.gz"

        if [ $? -ne 0 ]; then
          echo "Error downloading backups from S3"
          exit 1
        fi

        echo "Force reinitializing PostgreSQL database..."
        /opt/scripts/postgresql-initdb.sh --force "$DATA_MOUNTPOINT"

        # Wait for PostgreSQL to be ready
        echo "Waiting for PostgreSQL to be ready..."
        until pg_isready -h $IP -U $DB_USER -q; do
          sleep 0.5
        done
        echo "PostgreSQL is ready!"

        # Restore global objects first
        if [[ -f "$BACKUP_DIR/globals_roles_privileges_backup.sql.gz" ]]; then
          echo "Restoring global roles and privileges..."
          zcat "$BACKUP_DIR/globals_roles_privileges_backup.sql.gz" | psql -h $IP -U $DB_USER -d postgres
        fi

        # Restore individual databases
        for BACKUP_FILE in "$BACKUP_DIR"/*.dump; do
          DB_NAME=$(basename "$BACKUP_FILE" .dump)
          echo "Restoring database $DB_NAME..."

          # Restore database
          pg_restore -h $IP -U $DB_USER -d postgres --clean --create --if-exists --verbose "$BACKUP_FILE"

          if [ $? -ne 0 ]; then
            echo "Warning: Some errors occurred while restoring database $DB_NAME"
          fi
        done

        rm -f "$BACKUP_DIR"/*.dump
        rm -f "$BACKUP_DIR"/*.gz
        rm -f "$PGPASSFILE"
        echo "Restore completed successfully."
      }

      # Execute command
      case "$COMMAND" in
        --create)
          create_backup "$ARGUMENT"
          ;;
        --list)
          list_backups
          ;;
        --restore)
          restore_backup "$ARGUMENT"
          ;;
        *)
          echo "Error: Unknown command $COMMAND"
          exit 1
          ;;
      esac
  - path: /opt/scripts/psql.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # Configuration
      DB_USER="${db_username}"
      SSM_PARAM="${ssm_db_credentials}"

      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"

      # Fetch connection string from AWS SSM Parameter Store
      echo "Fetching connection string from AWS SSM Parameter Store..."
      CONNECTION_STRING=$(aws ssm get-parameter --name "$SSM_PARAM" --with-decryption --query 'Parameter.Value' --output text)

      # Extract password from the connection string
      PASSWORD=$(echo "$CONNECTION_STRING" | awk -F'[:@]' '{print $3}')

      # Check if the password retrieval was successful
      if [ -z "$PASSWORD" ]; then
        echo "Error: failed to retrieve password from SSM Parameter Store."
        exit 1
      fi

      # Get our primary IP address
      IP=$(ip route get 1.1.1.1 | grep -oP '(?<=src )\S+')

      # Create a temporary file for storing the password
      PGPASSFILE=$(mktemp)
      echo "$IP:*:*:$DB_USER:$PASSWORD" > "$PGPASSFILE"
      chmod 600 "$PGPASSFILE"

      # Set environment variable to use the password file
      export PGPASSFILE

      # Run psql
      psql -h $IP -U $DB_USER ${db_name}
  - path: /etc/systemd/system/postgresql-backup.service
    permissions: '0700'
    content: |
      [Unit]
      Description=PostgreSQL Database Backup

      [Service]
      Type=oneshot
      ExecStart=/opt/scripts/postgresql-backup.sh
  - path: /etc/systemd/system/postgresql-backup.timer
    permissions: '0700'
    content: |
      [Unit]
      Description=PostgreSQL Backup Timer (8:00 - 10:00 AM Window)

      [Timer]
      OnCalendar=*-*-* 08:00:00
      RandomizedDelaySec=2h
      Persistent=true

      [Install]
      WantedBy=timers.target
  - path: /opt/scripts/deploy-app-in-docker.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      WORKDIR=$${WORKDIR:-/srv/code}
      GIT_REPOSITORY=$${GIT_REPOSITORY:-${git_repository}}
      DEFAULT_BRANCH=$${DEFAULT_BRANCH:-main}

      setup_ssh_config() {
        local repo_deploy_key=$(aws ssm get-parameter --name "${ssm_repo_deploy_key}" --with-decryption --query 'Parameter.Value' --output text)
        mkdir -p ~/.ssh
        chmod 700 ~/.ssh
        echo -e "$repo_deploy_key" > ~/.ssh/id_rsa
        chmod 600 ~/.ssh/id_rsa
        ssh-keyscan -t rsa $(echo "$GIT_REPOSITORY" | grep -oP '(?<=@)[^:]+') > ~/.ssh/known_hosts
      }

      sync_sources() {
        # clone repo if WORKDIR is not a valid git repo
        git -C $WORKDIR rev-parse 2>/dev/null || git clone $GIT_REPOSITORY $WORKDIR

        cd $WORKDIR

        # always hard reset and clean before any checkout to avoid checkout errors
        git reset --hard
        git clean -fd

        git fetch --all

        # determine commit/branch to reset to
        COMMIT="origin/$DEFAULT_BRANCH"
        [[ -n "$${COMMIT_HASH:-}" ]] && COMMIT="$COMMIT_HASH"

        # force reset working tree and index to exact commit state
        git reset --hard "$COMMIT"
      }

      configure_host() {
        [[ -s .deploy/ec2-configure-host.sh ]] && .deploy/ec2-configure-host.sh || return 0
      }

      prepare_docker_aws() {
        ECR_REGISTRY=$(aws ecr get-authorization-token --query authorizationData[0].proxyEndpoint --output text | cut -d/ -f3)
        aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_REGISTRY
      }

      prepare_docker_overrides() {
        [[ -z "$${COMPOSE_OVERRIDES:-}" ]] && { echo "COMPOSE_OVERRIDES not set, skipping"; return 0; }

        for file in $${COMPOSE_OVERRIDES[@]}; do
          [[ ! -s "$file" ]] && { echo "File $file not found, skipping"; return 0; }
        done

        docker run --rm -v "$PWD":/workdir mikefarah/yq ea '. as $item ireduce ({}; . * $item)' \
          $${COMPOSE_OVERRIDES[@]} > docker-compose.override.yml
      }

      setup_env_file() {
        [[ -s .env ]] && mv .env .env.bak

        ENV_FILES="${ssm_env_files}"
        for env_file in $ENV_FILES; do
          aws ssm get-parameter --name "$env_file" --with-decryption --query 'Parameter.Value' --output text >> .env
        done
      }

      run_docker() {
        docker system prune -f
        docker compose pull --ignore-pull-failures -q
        docker compose build -q
        docker compose up -d
      }

      setup_ssh_config
      sync_sources
      configure_host
      prepare_docker_aws
      prepare_docker_overrides
      setup_env_file
      run_docker
runcmd:
  # install aws ssm agent
  - yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm
  - systemctl enable --now --no-block amazon-ssm-agent
  # dist-upgrade
  - dnf upgrade -y --refresh --releasever=latest
  # mounting ebs disks
  - /opt/scripts/ebs-mount.sh ${data_device_name} ${data_mountpoint} ${data_service_name}
  - /opt/scripts/ebs-mount.sh ${backup_device_name} ${backup_mountpoint} ${backup_service_name}
  # download docker-compose
  - systemctl enable --now --no-block --force docker
  - /opt/scripts/get-docker-extras.sh
  - systemctl disable --now --no-block --force docker
  # initialize postgresql server (separated from "packages:" block)
  - yum install -y postgresql16-server postgresql16-contrib
  - /opt/scripts/postgresql-initdb.sh ${data_mountpoint}
  - systemctl enable --now --no-block --force postgresql@${stage}
  - systemctl daemon-reload
  - systemctl enable --now --no-block postgresql-backup.timer
  # deploy application
  - /opt/scripts/deploy-app-in-docker.sh
