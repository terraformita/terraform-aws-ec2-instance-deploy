#cloud-config
output: { all : '| tee -a /var/log/cloud-init-output.log' }
repo_update: true
repo_upgrade: all
package_upgrade: true
package_reboot_if_required: true

cloud_init_modules:
  - migrator
  - bootcmd
  - write-files
  - growpart
  - resizefs
  - disk_setup
  - mounts
  - set_hostname
  - update_hostname
  - update_etc_hosts
  - ca-certs
  - rsyslog
  - users-groups
  - ssh

# AL2023
packages:
  - bash-completion
  - e2fsprogs
  - ec2-utils
  - git-core
  - vim-enhanced
  - awscli
  - nvme-cli
  - jq
  - htop
  - docker
  - cronie
  - pv

write_files:
  - path: /opt/scripts/ebs-mount.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      device_name=$${device_name:-$1}
      mountpoint=$${mountpoint:-$2}
      service_name=$${service_name:-$3}

      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"
      AVAILABILITY_ZONE="$(ec2-metadata --availability-zone | awk '{print $2}')"
      INSTANCE_ROLE="$(aws ec2 describe-tags --filters Name=resource-id,Values=$(ec2-metadata --instance-id | awk '{print $2}') | jq -r '.Tags[] | select(.Key == "Role") | .Value')"
      echo "AVAILABILITY_ZONE: $AVAILABILITY_ZONE"
      echo "INSTANCE_ROLE: $INSTANCE_ROLE"

      while true; do
          i=1
          while UNATTACHED_VOLUME_ID="$(aws ec2 describe-volumes --filters Name=tag:Service,Values=$service_name Name=tag:AutoAttachGroup,Values=$INSTANCE_ROLE Name=availability-zone,Values=$AVAILABILITY_ZONE | jq -r '.Volumes[] | select(.Attachments | length == 0) | .VolumeId' | shuf -n 1)";
          do
            if [[ -n $UNATTACHED_VOLUME_ID ]] || [[ $i -gt 3 ]]; then break; fi
            echo [$i/3] UNATTACHED_VOLUME_ID is empty, waiting for 2 minutes
            sleep 120 && i=$((i+1))
            continue
          done

          echo "UNATTACHED_VOLUME_ID: $UNATTACHED_VOLUME_ID"

          aws ec2 attach-volume --device "$device_name" --instance-id=$(ec2-metadata --instance-id | awk '{print $2}') --volume-id $UNATTACHED_VOLUME_ID
          if [ "$?" != "0" ]; then
              sleep 10
              continue
          fi

          sleep 30

          ATTACHMENTS_COUNT="$(aws ec2 describe-volumes --filters Name=volume-id,Values=$UNATTACHED_VOLUME_ID | jq -r '.Volumes[0].Attachments | length')"
          if [ "$ATTACHMENTS_COUNT" != "0" ]; then break; fi
      done
      echo 'Waiting for 30 seconds for the disk to become mountable...'
      sleep 30

      mkdir -vp $mountpoint
      export MOUNTED_DEVICE_NAME=$(lsblk -ip --noheadings | awk '{print $1 " " ($7? "MOUNTEDPART" : "") }' | sed ':a;N;$!ba;s/\n`/ /g' | grep -v MOUNTEDPART | grep -v /dev/nvme0n1)
      echo $MOUNTED_DEVICE_NAME
      if mount -o defaults -t ext4 $device_name $mountpoint; then
          echo 'Successfully mounted existing disk'
      else
          echo 'Trying to mount a fresh disk'
          mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard $device_name
          mount -o defaults -t ext4 $device_name $mountpoint && echo 'Successfully mounted a fresh disk'
      fi
      echo "$device_name $mountpoint ext4 defaults,nofail 0 2" | tee -a /etc/fstab
  - path: /opt/scripts/get-docker-extras.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -eu -o pipefail

      # install old docker-compose
      yum install -y libxcrypt-compat
      version=1.29.2
      os=$(uname -s)
      arch=$(uname -m)

      ln -s /usr/share/bash-completion/docker /etc/bash_completion.d/docker

      # install new compose plugin
      version=2.32.0
      plugins_dir=/usr/local/lib/docker/cli-plugins
      mkdir -p $plugins_dir
      curl -SL https://github.com/docker/compose/releases/download/v$version/docker-compose-$${os,,}-$arch -o $plugins_dir/docker-compose
      chmod +x $plugins_dir/docker-compose

  - path: /opt/scripts/postgresql-initdb.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # Set default paths
      MOUNTPOINT=$${mountpoint:-$1}
      PGDIR=$MOUNTPOINT/pgsql
      DATADIR=$PGDIR/data

      # Create dirs
      if [ ! -d $PGDIR ]; then
        mkdir -p $PGDIR
        chown "postgres:" $PGDIR
      fi

      # Create systemd unit file
      if [ ! -s /etc/systemd/system/postgresql@${stage}.service.d/30-postgresql-setup.conf ]; then
        postgresql-new-systemd-unit --unit=postgresql@${stage} --datadir=$DATADIR
      fi

      if [ -d $DATADIR ]; then
        echo "Datadir '$DATADIR' already exists, exiting..."
        exit 0
      fi

      SSM_PARAM="${ssm_db_credentials}"
      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"

      # Fetch the connection string from AWS SSM Parameter Store
      CONNECTION_STRING=$(aws ssm get-parameter --name "$SSM_PARAM" --with-decryption --query 'Parameter.Value' --output text)

      # Extract password and port from the connection string
      PASSWORD=$(echo "$CONNECTION_STRING" | awk -F'[:@]' '{print $3}')
      PORT=$(echo "$CONNECTION_STRING" | awk -F'[:/]' '{print $6}')

      # Check if the password retrieval was successful
      if [ -z "$PASSWORD" ]; then
        echo "Failed to retrieve password from AWS Secrets Manager"
        exit 1
      fi

      # Create a temporary file for the password
      PWFILE=$(mktemp)
      echo "$PASSWORD" > "$PWFILE"
      # Set secure permissions on the file
      chmod 600 "$PWFILE"
      chown "postgres:" "$PWFILE"

      # Initialize PostgreSQL with the password file
      export PGSETUP_INITDB_OPTIONS="--pgdata=$DATADIR --pwfile=$PWFILE"
      postgresql-setup --unit postgresql@${stage} --port $PORT --initdb

      # Set the server configuration
      sed -i "s/^#\(listen_addresses = \)\('localhost'\)/\1'*'/" $DATADIR/postgresql.conf

      # Setup ssl certificates
      SSL_DIR=${ssl_certs_dest}
      mkdir -p "$SSL_DIR"
      CERTS=( ${ssl_certs} )
      for cert in "$${CERTS[@]}"
      do
        aws ssm get-parameter --name ${ssl_certs_ssm_prefix}/$cert --with-decryption --query 'Parameter.Value' --output text > $SSL_DIR/$cert
      done

      chown -R postgres:postgres $SSL_DIR
      chmod 600 $SSL_DIR/*

      # SSL configuration
      echo "ssl = on" >> $DATADIR/postgresql.conf
      echo "ssl_ca_file = '${ssl_certs_dest}/ca.pem'" >> $DATADIR/postgresql.conf
      echo "ssl_cert_file = '${ssl_certs_dest}/server-cert.pem'" >> $DATADIR/postgresql.conf
      echo "ssl_key_file = '${ssl_certs_dest}/server-key.pem'" >> $DATADIR/postgresql.conf

      # Allow SSL-only network authorization for user postgres
      echo "hostssl all             postgres        ${db_allowed_network}           scram-sha-256" >> $DATADIR/pg_hba.conf
      echo "hostssl ${db_name}             ${db_username}        ${db_allowed_network}           scram-sha-256" >> $DATADIR/pg_hba.conf
      echo "hostssl ${db_name}             ${db_username}        172.16.0.0/12           scram-sha-256" >> $DATADIR/pg_hba.conf

      # start postgresql server
      systemctl enable --now --no-block --force postgresql@${stage}

      # Create the application database and set permissions
      echo "CREATE DATABASE ${db_name};" | su - postgres -c "psql"
      echo "CREATE USER ${db_username} WITH PASSWORD '$(cat $PWFILE)';" |  su - postgres -c "psql"
      echo "GRANT ALL PRIVILEGES ON DATABASE ${db_name} TO ${db_username};" |  su - postgres -c "psql"
      echo "ALTER DATABASE ${db_name} OWNER TO ${db_username};" | su - postgres -c "psql"
      echo "ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO ${db_username};" | su - postgres -c "psql ${db_name}"

      # Clean up the password file
      rm -f "$PWFILE"

      echo "PostgreSQL initialization completed."

  - path: /opt/scripts/postgresql-backup.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # Configuration
      SERVICE="${service_name}"
      DB_USER="${db_backup_user}"
      SSM_PARAM="${ssm_db_credentials}"
      S3_BUCKET="${db_backup_bucket}"
      BACKUP_DIR="${db_backup_dir}"
      TIMESTAMP=$(date +'%Y/%m/%d')

      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"

      # Fetch connection string from AWS SSM Parameter Store
      echo "Fetching connection string from AWS SSM Parameter Store..."
      CONNECTION_STRING=$(aws ssm get-parameter --name "$SSM_PARAM" --with-decryption --query 'Parameter.Value' --output text)

      # Extract password from the connection string
      PASSWORD=$(echo "$CONNECTION_STRING" | awk -F'[:@]' '{print $3}')

      # Check if the password retrieval was successful
      if [ -z "$PASSWORD" ]; then
        echo "Error: failed to retrieve password from SSM Parameter Store."
        exit 1
      fi

      # Get our primary IP address
      IP=$(ip route get 1.1.1.1 | grep -oP '(?<=src )\S+')

      # Create a temporary file for storing the password
      PGPASSFILE=$(mktemp)
      echo "$IP:*:*:$DB_USER:$PASSWORD" > "$PGPASSFILE"
      chmod 600 "$PGPASSFILE"

      # Set environment variable to use the password file
      export PGPASSFILE

      # Create a temporary directory for backups
      mkdir -p "$BACKUP_DIR"

      # Backup global roles and privileges
      echo "Creating a backup of global roles and privileges..."
      GLOBAL_BACKUP_FILE="globals_roles_privileges_backup.sql.gz"
      pg_dumpall -h $IP -U $DB_USER --globals-only | gzip > "$BACKUP_DIR/$GLOBAL_BACKUP_FILE"

      # Check if global objects backup was successful
      if [ $? -ne 0 ]; then
        echo "Error while creating backup of global roles and privileges."
        exit 1
      fi

      # Upload global backup to S3
      GLOBAL_S3_PATH="s3://$S3_BUCKET/$SERVICE/$TIMESTAMP/$GLOBAL_BACKUP_FILE"
      echo "Uploading global backup to $GLOBAL_S3_PATH..."
      aws s3 cp "$BACKUP_DIR/$GLOBAL_BACKUP_FILE" "$GLOBAL_S3_PATH"

      # Fetch the list of all databases
      DB_LIST=$(psql -h $IP -U $DB_USER -d postgres -t -c "SELECT datname FROM pg_database WHERE datistemplate = false;")

      # Backup each database
      for DB_NAME in $DB_LIST; do
        echo "Creating a backup for database '$DB_NAME'..."
        BACKUP_FILE="$${DB_NAME}.sql.gz"
        pg_dump -h $IP -U $DB_USER -F c -d $DB_NAME | gzip > "$BACKUP_DIR/$BACKUP_FILE"

        # Check if database backup was successful
        if [ $? -ne 0 ]; then
          echo "Error while creating backup for database '$DB_NAME'."
          exit 1
        fi

        # Upload database backup to S3
        S3_PATH="s3://$S3_BUCKET/$SERVICE/$TIMESTAMP/$BACKUP_FILE"
        echo "Uploading backup of database '$DB_NAME' to $S3_PATH..."
        aws s3 cp "$BACKUP_DIR/$BACKUP_FILE" "$S3_PATH"

        # Check if upload was successful
        if [ $? -eq 0 ]; then
          echo "Backup of database '$DB_NAME' successfully uploaded to $S3_PATH"
        else
          echo "Error uploading backup of database '$DB_NAME' to S3"
          exit 1
        fi
      done

      # Clean up temporary backup files and password file
      echo "Cleaning up local backup files and password file..."
      rm -f "$BACKUP_DIR"/*.gz
      rm -f "$PGPASSFILE"

      echo "Backup process completed."
  - path: /opt/scripts/psql.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # Configuration
      DB_USER="${db_username}"
      SSM_PARAM="${ssm_db_credentials}"

      export AWS_DEFAULT_REGION="$(ec2-metadata --availability-zone | awk '{print $2}' | rev | cut -c2- | rev)"

      # Fetch connection string from AWS SSM Parameter Store
      echo "Fetching connection string from AWS SSM Parameter Store..."
      CONNECTION_STRING=$(aws ssm get-parameter --name "$SSM_PARAM" --with-decryption --query 'Parameter.Value' --output text)

      # Extract password from the connection string
      PASSWORD=$(echo "$CONNECTION_STRING" | awk -F'[:@]' '{print $3}')

      # Check if the password retrieval was successful
      if [ -z "$PASSWORD" ]; then
        echo "Error: failed to retrieve password from SSM Parameter Store."
        exit 1
      fi

      # Get our primary IP address
      IP=$(ip route get 1.1.1.1 | grep -oP '(?<=src )\S+')

      # Create a temporary file for storing the password
      PGPASSFILE=$(mktemp)
      echo "$IP:*:*:$DB_USER:$PASSWORD" > "$PGPASSFILE"
      chmod 600 "$PGPASSFILE"

      # Set environment variable to use the password file
      export PGPASSFILE

      # Run psql
      psql -h $IP -U $DB_USER ${db_name}
  - path: /etc/systemd/system/postgresql-backup.service
    permissions: '0700'
    content: |
      [Unit]
      Description=PostgreSQL Database Backup

      [Service]
      Type=oneshot
      ExecStart=/opt/scripts/postgresql-backup.sh
  - path: /etc/systemd/system/postgresql-backup.timer
    permissions: '0700'
    content: |
      [Unit]
      Description=PostgreSQL Backup Timer (8:00 - 10:00 AM Window)

      [Timer]
      OnCalendar=*-*-* 08:00:00
      RandomizedDelaySec=2h
      Persistent=true

      [Install]
      WantedBy=timers.target
  - path: /opt/scripts/deploy-app-in-docker.sh
    permissions: '0700'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      WORKDIR=$${WORKDIR:-/srv/code}
      GIT_REPOSITORY=$${GIT_REPOSITORY:-${git_repository}}
      DEFAULT_BRANCH=$${DEFAULT_BRANCH:-main}

      setup_ssh_config() {
        local repo_deploy_key=$(aws ssm get-parameter --name "${ssm_repo_deploy_key}" --with-decryption --query 'Parameter.Value' --output text)
        mkdir -p ~/.ssh
        chmod 700 ~/.ssh
        echo -e "$repo_deploy_key" > ~/.ssh/id_rsa
        chmod 600 ~/.ssh/id_rsa
        ssh-keyscan -t rsa $(echo "$GIT_REPOSITORY" | grep -oP '(?<=@)[^:]+') > ~/.ssh/known_hosts
      }

      sync_sources() {
        # clone repo if WORKDIR is not a valid git repo
        git -C $WORKDIR rev-parse 2>/dev/null || git clone $GIT_REPOSITORY $WORKDIR

        cd $WORKDIR

        # always hard reset and clean before any checkout to avoid checkout errors
        git reset --hard
        git clean -fd

        git fetch --all

        # determine commit/branch to reset to
        COMMIT="origin/$DEFAULT_BRANCH"
        [[ -n "$${COMMIT_HASH:-}" ]] && COMMIT="$COMMIT_HASH"

        # force reset working tree and index to exact commit state
        git reset --hard "$COMMIT"
      }

      configure_host() {
        [[ -s .deploy/ec2-configure-host.sh ]] && .deploy/ec2-configure-host.sh || return 0
      }

      prepare_docker_aws() {
        ECR_REGISTRY=$(aws ecr get-authorization-token --query authorizationData[0].proxyEndpoint --output text | cut -d/ -f3)
        aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_REGISTRY
      }

      prepare_docker_overrides() {
        [[ -z "$${COMPOSE_OVERRIDES:-}" ]] && { echo "COMPOSE_OVERRIDES not set, skipping"; return 0; }

        for file in $${COMPOSE_OVERRIDES[@]}; do
          [[ ! -s "$file" ]] && { echo "File $file not found, skipping"; return 0; }
        done

        docker run --rm -v "$PWD":/workdir mikefarah/yq ea '. as $item ireduce ({}; . * $item)' \
          $${COMPOSE_OVERRIDES[@]} > docker-compose.override.yml
      }

      setup_env_file() {
        [[ -s .env ]] && mv .env .env.bak

        ENV_FILES="${ssm_env_files}"
        for env_file in $ENV_FILES; do
          aws ssm get-parameter --name "$env_file" --with-decryption --query 'Parameter.Value' --output text >> .env
        done
      }

      run_docker() {
        docker system prune -f
        docker compose pull --ignore-pull-failures -q
        docker compose build -q
        docker compose up -d
      }

      setup_ssh_config
      sync_sources
      configure_host
      prepare_docker_aws
      prepare_docker_overrides
      setup_env_file
      run_docker
runcmd:
  # install aws ssm agent
  - yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm
  - systemctl enable --now --no-block amazon-ssm-agent
  # dist-upgrade
  - dnf upgrade -y --refresh --releasever=latest
  # mounting ebs disks
  - /opt/scripts/ebs-mount.sh ${data_device_name} ${data_mountpoint} ${data_service_name}
  - /opt/scripts/ebs-mount.sh ${backup_device_name} ${backup_mountpoint} ${backup_service_name}
  # download docker-compose
  - systemctl enable --now --no-block --force docker
  - /opt/scripts/get-docker-extras.sh
  - systemctl disable --now --no-block --force docker
  # initialize postgresql server (separated from "packages:" block)
  - yum install -y postgresql16-server postgresql16-contrib
  - /opt/scripts/postgresql-initdb.sh ${data_mountpoint}
  - systemctl enable --now --no-block --force postgresql@${stage}
  - systemctl daemon-reload
  - systemctl enable --now --no-block postgresql-backup.timer
  # deploy application
  - /opt/scripts/deploy-app-in-docker.sh
